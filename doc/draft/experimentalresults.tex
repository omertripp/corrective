\section{Implementation and Evaluation}\label{Se:experiments}

In this section, we first describe our prototype implementation of the overall system (both the static analysis and the runtime synchronization algorithm), and then its experimental evaluation over a set of five real-world subjects. 

\subsection{Prototype System}

We have created a Java implementation of our static analysis for composed {\sf Map} operations, as described  in Section \ref{Se:instance}. Our implementation covers all the operations listed in Figure \ref{fig:language}. \pietrotodo{Move paragraph on $T_1,T_2,T_n$ over here.} 

As explained earlier, the interface with the runtime system is a relational warping specification mapping prestates to sets of poststates that are obtainable via serializable execution of the transactions from the prestate. As a partial example, 
\begin{center}
$[ {\sf k} \mapsto \bot , {\sf v} \mapsto v ] \leadsto \{ [ {\sf k} \mapsto v , {\sf v} \mapsto v ] \}$
\end{center}
denotes that in the poststate, the key ${\sf k}$ is made to point to the value $v$ pointed-to by the second argument ${\sf v}$ in the prestate.
%
The runtime system $S$ is parameterized by the specification, which it loads at the beginning of the concurrent run. 

As mentioned earlier, in our current prototype all transactions are assumed to start simultaneously, which is useful for the common case of loop parallelization. Each concrete transaction $t$ is mapped to its abstract counterpart $\overline{t}$, as explained in Section \ref{Se:concabs}. The mapping process also binds the concrete arguments of the transaction (i.e., the concrete object references) to their symbolic counterparts (e.g., the {\sf k} and {\sf v} symbols above). 

During execution, the runtime system monitors commit events. In our current prototype, we limit transactions to a single commit point before completion. Warping occurs only on failed commits (as discussed in Section \ref{Se:guarantees}), in which case the transaction's shared log, local state and return value (if it exists) are all (potentially) modified according to the warping specification. 

The specification is cast by the runtime system into a hierarchical representation, which conveys constraints between past and (potential) future warping decisions to ensure overall consistency toward converging on a correct poststate. This means in particular that as execution progresses, the number of possible warping targets for a given transaction decreases monotonically.

\subsection{Methodology and Experimental Setup}

We conducted our experiments on an X86\_64 ThinkPad W530 workstation with eight 2.30GHz Intel Core i7-3610QM processors, 16GB of RAM and 6M caches. The workstation runs version
12.04 of the Ubuntu Linux distribution, and has the Sun 64-Bit 1.7 Java virtual machine (JVM) installed.

In our experiments, we measured two aspects of parallel execution:
\begin{itemize}
	\item \underline{Workload size:} For a fixed number $n$ of threads, where $n=8$,  the workload size is varied. The workload is measured as \pengtodo{how is the workload measured?}.
	\item \underline{Concurrency level:} For a fixed workload size, set at \pengtodo{what is workload size?}, the number of threads is varied along the range of 1 to 32 threads.  
\end{itemize}
For statistical soundness, we report the average performance results across 10 independent repetitions of each experiment.

We compare our technique against two competing solutions:
\begin{itemize}
	\item a pessimistic concrete-level variant of STM, as available via version 1.3 of the Deuce STM (the latest version),\footnote{
		\url{https://github.com/DeuceSTM/DeuceSTM}
	}; and
	\item a lock-based synchronization algorithm boosted with {\sf Map} semantics \cite{EricBoosting}.
\end{itemize}

\subsection{Subjects}

Our benchmark suite consists of four subjects, all of which are taken from popular open-source code bases and have been used in past studies on concurrency \cite{OhadShacham,...}, as follows:
\begin{itemize}
	\item \underline{Apache Tomcat:} This is a web-application container in wide deployment. Within it, we consider {\sf ApplicationContext}, which is ...
	\item \underline{dyuproject:} This is a library for ... We experiment with {\sf StandardConvertorCache}, which ...
	\item \underline{Flexive:} The purpose of this library is to ... We have included {\sf FxValueRendererFactory} from it, which ...
	\item \underline{Gridkit:} This library provides .... In it, {\sf ReflectionPofSerializer} is a concurrent data structure for ...
\end{itemize}
Though the above subjects were all designed as concurrent data structures, they contain bugs that mandate additional synchronization \cite{OhadShacham}.

We have extracted two transactional methods out of each of the benchmarks. For example, XXX. In our experiments, each thread runs a XXX combination of the two transactions.

cases 3,4,5: after first successful transaction, remaining transactions will be read only (conditional update), so no conflict. stm does pretty good.

case 2: there's blind updates (no checking before), so stm is problematic. lots of conflicts.

\subsection{Performance Results} 

We depict the performance results in Figures \ref{XXX}-\ref{XXX}. 
For each benchmark, we present two graphs: The left-hand-side graphs correspond to the first measurement, where we increase the size of the workload while keeping the number of threads fixed at 8. The expected trend for these graphs is monotonic increase in running time. The right-hand-side graphs correspond to a fixed-size workload executed by each of the transactions, where the number of threads is variable. Here, too, we expect an increase in running time due to interference between threads.

We omit the trend line for lock-based synchronization from the graphs, but present the respective data in tabular form (in Table \ref{XXX}), since in all four cases the lock-based solution follows a linear trend of degradation (as a function of both workload size and the number of threads). Excluding its trend line from the graph enables a more meaningful scale to compare between STM and warping.

The results are consistent with our expectations. Across all benchmarks, both the (a) and the (b) graphs largely follow the expected trend. Further, in all cases the warping approach achieves the least running time. The most significant gap is w.r.t. benchmark XXX, where for the largest workload our approach is x5 more performant than STM (the (a) graph), and for 32 threads the improvement compared to STM is x6 (the (b) graph).

We summarize the comparison between STM, warping and  lock-based synchronization in Table \ref{XXX}. We indicate both absolute running times and relative improvement.

In Figure~\ref{fig:case2Iter}, the X axis is the number of transactions in each thread, Y axis is the running time (unit: milliseconds). We fix the number of threads as 8. Although the number of transactions changes, the ratio between the $lock/stm$ time and the $warp$ time remains around 5, i.e., the $warp$ version runs as  5X times as the $lock/stm$ versions. We interpret the results in the following ways. First, the $lock$ version essentially serialize all the transaction instances because they involve the same key. The $stm$ version also exhibits the  parallelism embarrassingly in this case: Many transaction instances unconditionally update the location of the key in the map, which conflict with the transaction instances that read the location and force them to rollback. The $stm$ and $lock$ versions have similar performance because they  both serialize the transaction instances. Second, the speedup of our approach over $lock/stm$ is fixed when the thread number is fixed, indicating that the speedup is subject to the number of threads. 



In Figure~\ref{fig:case2Th}, the X axis is the number of threads, Y axis is the running time. We fix the number of transactions each thread runs as 100. The ratio between the $lock/stm$ time and the $warp$ increases when the number of threads increases. For example, the $lock/stm$ version takes around 2X time as the $warp$ with 2 threads, around 3X time with 4 threads, around 5X with 8 threads, and remain 5X with more than 8 threads. This observation confirms our previous conclusion that the speedup is subject to the the number of threads. In this case, we get the best the speedup with 8 threads, where 8 is also the number of the cores, indicating that we make full exploitation of the cores with 8 threads. The figure shows that our approach has the dramatically better thread scalability than existing approaches. Another interesting observation is, when there is a single thread, the stm version takes 164 ms, while both our version and the lock version takes 112 ms. The stm takes longer time because the Deuce implementation instruments the classes on  the fly when they are loaded, which consumes extra time. 


Figure~\ref{fig:case3Iter}, Figure~\ref{fig:case4Iter} and Figure~\ref{fig:case4Iter} show the performance when the workload increases and follow the same setting as the test 2 shown in Figure~\ref{fig:case2Iter}. Similar to Figure~\ref{fig:case2Th}, Figure~\ref{fig:case3Th}, Figure~\ref{fig:case4Iter} and Figure~\ref{fig:case4Iter} show the performance when the number of threads increases. From the figures, we see the trend of the lock version and our version resemble case 2, but the stm version behaves quite differently. An interesting observation is, the time difference between the stm version and ours is almost a constant. This suggests that the stm version achieves the same parallelism degree as ours, while incurring some extra  overhead of class instrumentation. We inspected the code and confirmed this conclusion. In these cases, many transaction instances first check and then update conditionally. After the first instances update, the following instances will fail the checking and are degenerated to read-only transactions. The read-only transactions allow the maximal parallelism like ours. 

\subsection{Discussion}

{\bf Execution Summary\ } The static analysis phase computes a set of execution summaries, each representing a legal execution, which are used as the input of the dynamic analysis phase.
Each execution summary describes (i) the return value of each transaction instance and (ii) the final map state
 

In our experiments,  we have two threads running two types of transactions ($TX_1$ and $TX_2$) respectively. One exemplary execution summary is, $[key\rightarrow v^1_1, r^1_1:v^1_1, r^n_1:v^1_1, r^1_2:v^1_1, r^n_2:v^1_1]$, where $r^1_1$ is the symbolic form of the return value of $TX^1_1$ (1$st$ instance of $TX_1$), $v^1_1$ is the symbolic form of the value put by the transaction instance, and $r^1_1:v^1_1$ describes the return value of the transaction instance. The readers may notice $r^n_1$. This symbol represents the return value of the instances that are not explicitly specified.  Here $n$ is determined by the capability of the static analysis. At runtime, we track the instance order and use $n$ by default if the number is not specified in the summary. The tracking of the instance order requires us to synchronize the first $n-1$ instances. However, the $n$ is typically a small number, and the synchronization overhead is negligible. We do not need to synchronize starting from the $n$th instance because all these instances are represented as $n$ by default. In addition, $key\rightarrow v^1_1$ tells what the key is associated with in the final map state. 

The execution summary is not limited to the above basic case. In general, it is initial-state sensitive, schedule-oblivious and site-sensitive. 
First, the initial state about whether the key is mapped to some value affects the computation of the execution summaries. Therefore, we prefix each execution summary with a specific initial state, e.g., the state with initial key $[key^{init}\rightarrow v^{init}]$ or the state without initial key $[key^{init}\rightarrow void]$.
Second, our execution summary is designed to be oblivious of the schedules. The schedule-obliviousness frees us from tracking the schedules at runtime and avoids the high tracking overhead. Third, the execution summary is site-sensitive. A transaction $TX^1_1$ may put a value dynamically created at site $A$ to the map. We symbolically represent the value using the site and the occurrence of the site (inside the transaction instance), e.g., $A1$ in $TX^1_1$. The extent to which we can distinguish the occurrences is determined by the static analysis, e.g., how many iterations can be unrolled.


{\bf Runtime System\ }
The runtime works as follows. We first use the counters to track the transaction instance and assemble the symbolic return accordingly, e.g., $r^1_1$.
Then we search for the symbolic value in the execution summaries, e.g., $v^1_1$. Last, using the symbolic value as a key, we look up the cache, which is maintained to associate each symbolic value with a runtime value, for the concrete runtime value.

The second step, i.e., searching for symbolic value in the summaries, is challenging. The searching is demanded on the fly at the return of each transaction instance. 
However, the returns should be consistent with each other such that the warped execution represents a realistic execution. In the other words, the return values should be searched for in the same execution summary. To achieve this, we implement an on-the-fly pruning algorithm. At the first return, it finds the symbolic value in a randomly picked execution summary. After the value is picked, the execution summaries with different value for the return will be pruned, leading to a smaller solution space. 
The algorithm is iteratively applied. In addition, to achieve initial-state sensitivity, we also reduce the solution space based on the initial states. 

Another tricky issue is, at some return, the symbolic value found in the execution summaries may have not been associated with any concrete runtime value yet.
For example, at the return of $TX^1_2$, we find the returned value as $v^1_1$ but $TX^1_1$ has not been executed, then we cannot find the runtime value associated with $v^1_1$ in the cache. This case happens because the execution summary is computed for certain schedule, which differs from the current schedule. In this case, we apply the notify/wait primitives to synchronize the cache lookup and cache maintenance. Even more complex, the returned value may never be put into the cache if the value creation site is disabled in the current schedule, e.g., its guarding branch condition is false. We simply remove all the guarding branches for the creation site such that it is always executed. This simple strategy is based on the insight that we treat the transaction as a blackbox and we only care about its return values. It is unnecessary to preserve the internal program semantics.


We also implemented the optimization for the caching. From the summaries, we see that only the values put by the first few transactions are used. Therefore, we only record such values into the cache and discard the rest at runtime.






\subsection{Evaluation}

%TODO: check if we need to remove teh business op from the warp version.
For performance measurement, we compare 4 versions with different types of synchronizations, the original version $orig$, the abstract key locking version $key$, the software transactional memory version $stm$, and the warping version $warp$.  The original version is without any synchronization, which may produce incorrect outputs. 
We use this version to approximate the best performance any synchronization can achieve. The abstract key locking protects each key with a unique lock~\cite{}. As compared to protecting the whole map with a single lock, it allows parallelism among the invocations with different keys. For the software transactional memory, we use the open sourced implementation {\sf Deuce}~\cite{}. {\sf Deuce} features the easiness of use. {\sf Deuce} is essentially a Java agent that instruments the classes during the class loading to insert the STM functionality. However, the JDK classes are loaded before the agent starts to work. Therefore, we re-import the JDK implementation as application classes, which {\sf Deuce} successfully apply to. 
The $warp$ version adopts our algorithm. In our evaluation, we use both the initial state with the key and the initial state without the key.  For space reason and clarity of figure, we only show the result based on the initial state with the key (other results are put into the technique report).


Our benchmark is adapted from real world applications, including XXX and YYY. In our experiment, we consider two types of transactions for each benchmark, and each thread runs a combination of the transactions. In our experiment, we measure the performance under two changes: (1) We fix the number of threads as 8, the number of cores in the machine, and then change the number of transactions each thread runs from 10 to 100. This setting helps us understand the performance in different workloads, (2) we fix the number of transactions each thread runs as 100, and change the number of threads following the sequence: 1,2,4,6,8,10,12,14,16. This setting allows us to study the thread scalability. In our experiments, all threads operate on the same key.

\begin{figure*}
	\begin{minipage}{0.45 \textwidth}
		\includegraphics[trim=190 190 190 190, width=\textwidth]{../../eval/32threads/case1it.pdf}
		\caption{\label{Fi:case1it}Apache Tomcat {\sf ApplicationContext}: workload}
	\end{minipage}
	\hspace{0.1 \textwidth}
	\begin{minipage}{0.45 \textwidth}
		\includegraphics[trim=190 190 190 190, width=\textwidth]{../../eval/32threads/case1th.pdf}
				\caption{\label{Fi:case1th}Apache Tomcat {\sf ApplicationContext}: concurrency}
	\end{minipage}
	\begin{minipage}{0.45 \textwidth}
		\includegraphics[trim=190 190 190 190, width=\textwidth]{../../eval/32threads/case2it.pdf}
						\caption{\label{Fi:case2it}dyuproject {\sf StandardConvertorCache}: workload}
	\end{minipage}
	\hspace{0.1 \textwidth}
	\begin{minipage}{0.45 \textwidth}
		\includegraphics[trim=190 190 190 190, width=\textwidth]{../../eval/32threads/case2th.pdf}
						\caption{\label{Fi:case2th}dyuproject {\sf StandardConvertorCache}: concurrency}
	\end{minipage}
	\begin{minipage}{0.45 \textwidth}
		\includegraphics[trim=190 190 190 190, width=\textwidth]{../../eval/32threads/case3it.pdf}
			\caption{\label{Fi:case3it}Flexive {\sf FxValueRendererFactory}: workload}
	\end{minipage}
		\hspace{0.1 \textwidth}
	\begin{minipage}{0.45 \textwidth}
		\includegraphics[trim=190 190 190 190, width=\textwidth]{../../eval/32threads/case3th.pdf}
			\caption{\label{Fi:case3th}Flexive {\sf FxValueRendererFactory}: concurrency}
	\end{minipage}
	\begin{minipage}{0.45 \textwidth}
		\includegraphics[trim=190 190 190 190, width=\textwidth]{../../eval/32threads/case4it.pdf}
		\caption{\label{Fi:case4it}Gridkit {\sf ReflectionPofSerializer}: workload}
	\end{minipage}
	\hspace{0.1 \textwidth}
	\begin{minipage}{0.45 \textwidth}
		\includegraphics[trim=190 190 190 190, width=\textwidth]{../../eval/32threads/case4th.pdf}
		\caption{\label{Fi:case4th}Gridkit {\sf ReflectionPofSerializer}: concurrency}
	\end{minipage}
\end{figure*}

%\begin{figure*}
%\centering
%\subfloat[{Case 2 with the increasing workload}]{\label{fig:case2Iter}\includegraphics[width=0.33\textwidth]{../../eval/case2-varyIter.pdf}}
%\subfloat[{Case 2 with the increasing number of threads}]{\label{fig:case2Th}\includegraphics[width=0.33\textwidth]{../../eval/case2-varyThreads.pdf}}
%\\
%\subfloat[{Case 3 with the increasing workload}]{\label{fig:case3Iter}\includegraphics[width=0.33\textwidth]{../../eval/case3-varyIter.pdf}}
%\subfloat[{Case 3 with the increasing number of threads}]{\label{fig:case3Th}\includegraphics[width=0.33\textwidth]{../../eval/case3-varyThreads.pdf}}
%\\
%\subfloat[{Case 4 with the increasing workload}]{\label{fig:case4Iter}\includegraphics[width=0.33\textwidth]{../../eval/case4-varyIter.pdf}}
%\subfloat[{Case 4 with the increasing number of threads}]{\label{fig:case4Th}\includegraphics[width=0.33\textwidth]{../../eval/case4-varyThreads.pdf}}
%\\
%\subfloat[{Case 5 with the increasing workload}]{\label{fig:case5Iter}\includegraphics[width=0.33\textwidth]{../../eval/case5-varyIter.pdf}}
%\subfloat[{Case 5 with the increasing number of threads}]{\label{fig:case5Th}\includegraphics[width=0.33\textwidth]{../../eval/case5-varyThreads.pdf}}
%\caption{Performance Measurement}
%\label{fig:perf}
%\vspace{-1em}
%\end{figure*}







